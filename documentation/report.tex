%-------------------------------------------------------------------------------
%	NAME:	report.tex
%	AUTHOR: Connor Beardsmore - 15504319
%	LAST MOD: 01/04/17
%	PURPOSE:	AMI Assignment Report
%	REQUIRES:	NONE
%-------------------------------------------------------------------------------

\documentclass[]{article}
\usepackage[ margin=3cm ]{geometry}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{hyperref}
\usepackage{transparent}
\usepackage{pdfpages}
\usepackage[style=chicago-authordate,backend=biber]{biblatex}

\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amssymb}

\pagestyle{fancy}
\fancyhf{}
\lhead{Connor Beardsmore - 15504319}
\rhead{AMI300}
\lfoot{May 2017}
\rfoot{\thepage}

\pagenumbering{arabic}
\graphicspath{{./images/}}

\addbibresource{bib/references.bib}
\nocite{*}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------
% OFFICIAL COVER PAGE

\includepdf[]{coverpage.pdf}
	
%-------------------------------------------------------------------------------
% TITLE PAGE

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		\LARGE\textbf{AMI300 Report} \vspace{0.5cm}
		\break
	    Informed Beam and SMA* Search Implementations
		\vspace{1cm}
		\break
		\Large\textbf{Connor Beardsmore - 15504319} 
		\vspace{15cm}

		\normalsize
		Curtin University \\
		Science and Engineering \\
		Perth, Australia \\
	    May 2017
	    
	\end{center}
\end{titlepage}

%-------------------------------------------------------------------------------
% AFFINE CIPHER

\vspace*{-0.8cm}
\begin{center}
	\section*{Informed Beam Search}
\end{center}

\vspace*{0.8cm}
\subsection*{Design Decisions}

The informed beam search is an incomplete, non-optimal search technique based on an admissible heuristic measure. The cost of each node is determined as $f(n)=h(n)$, thus the decision of which nodes to expand is based solely on heuristic cost. The algorithm tracks up to $k$ beams or paths at each step. Each further step expands all children nodes from these beams and expands the best $k$ choices. Informed beam search sacrifices optimality and completeness for increased memory efficiency and speed (\cite{winston}) in comparison to A*.\\

The design of the algorithm is based around the explanation provided by \cite{winston}. The algorithm implemented utilizes a priority queue for both the \textit{beam} and the \textit{frontier} data structures, sorted via increasing heuristic cost. The \textit{beam} represents the current nodes in the $k$ beams while the \textit{frontier} stores all children of every node in the beam. At each level, the frontier is trimmed to $k$ length to allow the best $k$ choices to be chosen. After this stage, the beam is replaced by the frontier and the process repeats until a solution is found, or no valid paths can be explored. \\

The algorithm allows for the discovery of alternate paths after an initial solution is discovered. Once the goal node is discovered in the frontier, the path is stored and the goal removed from the frontier to allow the beams to continue as if the goal was never discovered. The beams will continue until they reach a dead end, or end up attemping to loop on themselves. This will eventually cause the beam to be empty and the algorithm will finish execution.

\subsection*{Problems and Bugs}

Several issues were faced during the implementation and testing of the algorithm. However, these issues were effectively resolved and the algorithm currently has no known issues or bugs resulting in errors. The following discusses how the issues faced were resolved.\\

The beams in the search technique do not communicate, they progress independently. Initially, the beams were communicating via comparing successors as they were added to the frontier. This fault was simply resolved by not adding duplicate nodes with the same path to the frontier at any stage. Furthermore, two beams can effectively converge at the same node whilst having different paths to reach that node. Initially, nodes were not being duplicated and paths were being overwritten and lost. To solve this problem, nodes were duplicated to save their specific individual paths to allow for beams to converge upon the same nodes and still produce different final paths to a potential solution.\\

Before improvements were applied, the list of partial paths stored when a solution is discovered was not being correctly displayed. Similarly to other issues, this was resolved by creating a deep copy of the beam before it was modified to allow for partial paths to be displayed if required. Printing of these partial paths has been tested to work on any $k$ beam width.\\

\subsection*{Testing and Efficiency}

Testing was performed on the algorithm with a number of graph files. These include the Romania graph from \cite{norvig}, the graphs from the tutorial sessions and script generated graphs of up to 5000 nodes. Beam values of between 1 and 20 were also tested to ensure no failure of the implementation upon scaling.\\

On a graph of 5000 nodes, 250,000 edges and a beam width of 3, the algorithm takes 2.1 seconds to find a solution path of depth 26 deep. To extend the algorithm to continue to find alternate paths, the time complexity scales poorly to 50.6 seconds. Alteration of the beam width did not have a significant impact on the time complexity to find a singular solution. However, while continuing to search for alternate solutions, the time scales exponentially as the beam width increases as is expected due to more of the graph being explored.

\vspace{1cm}
\begin{center}
	\section*{Simplified Memory Limited A* Search}
\end{center}

\vspace*{0.8cm}
\subsection*{Design Decisions}

The simplified memory limited A* search (SMA*) is an extension to pure memory bounded A* search, presented by Stuart Russell \cite{russell_paper}, based on insight form \cite{chakra}. It provides a more memory efficient form of the regular A* search by placing a cap on the number of nodes in memory at any one time. Like A* search, the evaluation function for a given node is defined as $f(n)=g(n)+h(n)$, thus being the sum of accumulated path cost and heuristic cost. It will produce the optimal solution given an admissible and consistent heuristic (\cite{norvig}) and solves the memory issues faced by regular A* at a cost of repeated calculations. \\

The algorithm makes use of two main data structures. The \textit{frontier} (open list) stores a list of all nodes currently stored in memory allowing the search to limit memory effectively. The \textit{leafNodes} list is a subset of the \textit{frontier} list which contains nodes that are leaves and have no children currently in the frontier. These lists are sorted by increasing $f(n)$ cost and decreasing depth. At each iteration the node to be opened will be the front node of the frontier, thus the node with lowest $f(n)$ cost and highest depth. If memory is ever full, the last node in the leafNodes list will be backed-up, thus the node with highest $f(n)$ cost and shallowest depth. Only leaf nodes are ever backed up and removed from memory.\\

The implementation was based upon the pseudo code presented in \cite{russell_paper}. This pseudo code is extremely limited and major parts withheld and thus the implementation differs significantly in some areas. The pseudo code removes parent nodes when all children are in memory and re-adds the parent if any of its children are ever backed up. This step provides no benefit to memory usage and was hence removed in the implementation. The successor function was also heavily neglected in the pseudo code and was a key component of the SMA* algorithm.\\

Similarly to the A* technique, upon finding a possible solution the search cannot immediately be sure it is optimal. In this situation it has to expand all current paths in memory until their $f(n)$ cost is greater then that of the current goal cost. This will ensure that the optimal solution is not missed. This was achieved by simply setting a bound when a solution is discovered and only completing the execution when the frontier reaches this bound. As a result of this, the algorithm by default will find all optimal paths.

\pagebreak
\subsection*{Problems and Bugs}

While the underlying theory of the algorithm is relatively simple, the amount of bookkeeping required to implement it correctly is significant (\cite{norvig}). At the current state, the implementation will correctly find a solution on the vast majority of graphs. However, this solution may not be optimal due to the following issue. On graphs with both consistent and admissible heuristics the situation rarely arises.  \\

The issue is a result of the decision made regarding duplicate nodes in the search tree. When a node is reached that has already been added to the frontier, it is skipped over and ignored. As a result, if a node is reached the first time at a path which is not optimal to that node, an optimal solution may never be visited in the tree. This kind of \textit{virtual pruning} should never be applied in SMA*. This does not alter the algorithms completeness, but does alter its optimality in certain graphs. Attempts to mitigate this issue via duplicating nodes in the search tree caused further issues with paths looping and thus were not employed. A solution to this issue is to simply replace the duplicate node with the more optimal path to that node, mending all subtrees and $f(n)$ costs in the graph to reflect the change.\\

Other problems were faced during implementation but were resolved to ensure the algorithms solutions were not affected. The main issues faced resolved around backing up nodes and updating the value in the parent that stores its best child. This information must be propagated up the entire tree to ensure the values are consistent with what has been observed. Attempting this recursively caused issues with looping and so, a simple approach to back up the cost whenever a node is removed worked, and fit well with how the algorithm naturally progresses $f(n)$ costs.\\

The final issue resolved around the successor function and how to maintain state about how many of a nodes children have been looked at at any given point. This issue was resolved by maintaining a simple counter of which children have been viewed in each node. When this value is greater than the number of children, the node is considered \textit{complete} and can be reset via updating the \textit{best} counter to represent its best childs $f(n)$ cost.

\subsection*{Testing and Efficiency}

The algorithm was tested utilizing the same graphs as for beam search. An implementation of regular A* was also utilized to ensure that SMA* was correctly determining paths that were optimal. To find the optimal solution in the same graph utilized for testing beam search, a total of 2800 iterations were required. This required approximately 2.1 seconds and in this example, SMA* and beam both provide a solution in the same time. However, the solution provided by SMA* is optimal in contrast to the non-optimal solution provided by beam search. The only time optimality is not maintained is when the optimal solution depth is greater than that of the nodes allowed in memory In this situation, the optimal path of the allowed depth will be determined instead, if possible.

%-------------------------------------------------------------------------------   
% REFERENCES

\break
\setlength\bibitemsep{4\itemsep}
\printbibliography[title={References}]

%-------------------------------------------------------------------------------
\end{document}   
%-------------------------------------------------------------------------------