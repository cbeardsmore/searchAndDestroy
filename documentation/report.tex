%-------------------------------------------------------------------------------
%	NAME:	report.tex
%	AUTHOR: Connor Beardsmore - 15504319
%	LAST MOD: 01/04/17
%	PURPOSE:	AMI Assignment Report
%	REQUIRES:	NONE
%-------------------------------------------------------------------------------

\documentclass[]{article}
\usepackage[ margin=3cm ]{geometry}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{hyperref}
\usepackage{transparent}
\usepackage{pdfpages}
\usepackage[style=chicago-authordate,backend=biber]{biblatex}

\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amssymb}

\pagestyle{fancy}
\fancyhf{}
\lhead{Connor Beardsmore - 15504319}
\rhead{AMI300}
\lfoot{May 2017}
\rfoot{\thepage}

\pagenumbering{arabic}
\graphicspath{{./images/}}

\addbibresource{bib/references.bib}
\nocite{*}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------
% OFFICIAL COVER PAGE

\includepdf[]{coverpage.pdf}
	
%-------------------------------------------------------------------------------
% TITLE PAGE

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		\LARGE\textbf{AMI300 Report} \vspace{0.5cm}
		\break
	    Informed Beam and SMA* Search Implementations
		\vspace{1cm}
		\break
		\Large\textbf{Connor Beardsmore - 15504319} 
		\vspace{15cm}

		\normalsize
		Curtin University \\
		Science and Engineering \\
		Perth, Australia \\
	    May 2017
	    
	\end{center}
\end{titlepage}

%-------------------------------------------------------------------------------
% AFFINE CIPHER

\vspace*{-0.8cm}
\begin{center}
	\section*{Informed Beam Search}
\end{center}

\vspace*{0.8cm}
\subsection*{Design Decisions}

The informed beam search is a non-complete and non-optimal search technique based on an admissible heuristic measure. The cost of each node is determined as $f(n)=h(n)$, thus the decision of which nodes to expand is based solely on heuristic cost. The algorithm tracks up to $k$ beams or paths at each step. Each further step expands all children nodes from these beams and expands the best $k$ choices. Informed beam search sacrifices optimality and completeness for increased memory efficiency and speed (\cite{winston}).\\

The design of the algorithm was based around the explanation provided by \cite{winston}. The algorithm implemented utilizes a priority queue for both the \textit{beam} and the \textit{frontier} data structures, sorted via increasing heuristic cost. The \textit{beam} represents the current nodes in the $k$ beams while the \textit{frontier} stores all children of every node in the beams. At each level the frontier is trimmed to $k$ length to allow the best $k$ choices to be chosen. After this stage, the beam is replaced by the frontier and the process repeats until a solution is found, or no valid paths can be explored. \\

The algorithm allows for the discovery of alternate paths after an initial solution is discovered. Once the goal node is discovered in the frontier, the path is stored and the goal removed from the frontier to allow the beams to continue as if the goal was never discovered. The beams will continue until they reach a dead end, or end up attemping to loop on themselves. This will eventually cause the beam to be empty and the algorithm will finish execution.

\subsection*{Problems and Bugs}

Several issues were faced during the implementation and testing of the algorithm. However, these issues were effectively resolved and the algorithm currently has no known issues or bugs resulting in errors. The following discusses how these issues were resolved.\\

The beams in the search technique do not communicate, they progress independently. Initially, the beams were communicating via comparing successors as they were added to the frontier. This fault was simply resolved by not adding duplicate nodes with the same path to the frontier at any stage. Two beams can effectively converge at the same node whilst having different paths to reach that node. Initially, nodes were not being duplicated and paths were being overwritten and lost. To solve this problem, nodes were duplicated to save their specific individual paths to allow for beams to converge upon the same nodes and still produce different final paths to a potential solution.\\

Before improvements were applied, the list of partial paths stored when a solution is discovered was not being correctly stored. Similarly to other issues, this was resolved by creating a deep copy of the beam before it was modified to allow for partial paths to be displayed if required.\\

\subsection*{Testing and Efficiency}

Testing was performed on the algorithm with a number of graph files. These include the Romania graph from \cite{norvig}, the graphs from the tutorial sessions and script generated graphs and heuristics of up to 5000 nodes. Beam values of between 1 and 20 were also tested to ensure no failure of the implementation.\\

On a graph of 5000 nodes, 250,000 edges and a beam width of 3, the algorithm takes 2.1 seconds to find a solution path of depth 26 deep. To extend the algorithm to continue to find alternate paths, the time complexity scales poorly to 50.6 seconds. Alteration of the beam width did not have a significant impact on the time complexity to find a singular solution. However, while continuing to search for alternate solutions, the time scales exponentially as the beam width increases as is expected due to more of the graph being explored.

\pagebreak

\begin{center}
	\section*{Simplified Memory Limited A* Search}
\end{center}

\vspace*{0.8cm}
\subsection*{Design Decisions}

The simplified memory limited A* search (SMA*) is an extension to pure memory bounded A* search, designed by Stuart Russell (\cite{russell_paper}). It provides a more memory efficient form of the regular A* search by placing a cap on the number of nodes in memory at any one time. Like A* search, the evaluation function for a given node is defined as $f(n)=g(n)+h(n)$, thus being the sum of accumulated path cost and heuristic cost. It will produce the optimal solution given an admissible and consistent heuristic (\cite{norvig}) and solves the memory issues faced by regular A*. \\

The algorithm makes use of two main data structures. The \textit{frontier} (open list) stores a list of all nodes currently stored in memory allowing the search to limit memory effectively. The \textit{leafNodes} list is a subset of the \textit{frontier} list which contains nodes that are leaves and have no children currently in the frontier. These lists are sorted by increasing $f(n)$ cost and decreasing depth. At each iteration the node to be opened will be the first in the frontier, thus the node with lowest $f(n)$ cost and highest depth. If memory is ever full, the last node in the leafNodes list will be backed-up, thus the node with highest $f(n)$ cost and shallowest depth.\\

The implementation was based off the pseudo code presented in \cite{russell_paper}. This pseudo code is extremely limited and major parts withheld and thus the implementation differs significantly in some areas. The pseudo code removes parent nodes when all children are in memory and re-adds the parent if any of its children are ever backed up. This step provides no benefit to memory usage and was hence removed in the implementation. The successor function was also heavily neglected in the pseudo code and was a key component of the SMA* algorithm.

\subsection*{Problems and Bugs}

While the underlying theory of the algorithm is relatively simple, the amount of bookkeeping required to implement it correctly is significant (\cite{norvig}). At the current state, the implementation will correctly find a solution on the vast majority of graphs. However, this solution may not be optimal due to the following issue. \\

The issue is a result of the decision made regarding duplicate nodes in the search tree. When a node is reached that is already been added to the frontier, it is skipped over and ignored. Thus, if a node is reached the first time at a path which is not optimal to that node, an optimal solution may never be visited in the tree. This does not alter the algorithms completeness but does alter its optimality in certain graphs. Attempts to mitigate this issue via duplicating nodes in the search tree caused further issues with paths looping and thus were not employed.\\

Other problems were faced during implementation but were resolved to ensure the algorithms solutions were not effected. The main issues were faced around backing up nodes and updating the value in the parent that stores its best child. This information must be propagated up the entire tree to ensure the values are consistent with what has been observed. Attempting this recursively caused issues with looping and thus, a simple approach to back up the cost whenever a node is removed worked and fit well with how the algorithm naturally progresses $f(n)$ costs.

\subsection*{Testing and Efficiency}

The algorithm was tested utilizing the same graphs as for beam search. An implementation of regular A* was also utilized to compare the paths for optimality.

%-------------------------------------------------------------------------------   
% REFERENCES

\break
\setlength\bibitemsep{4\itemsep}
\printbibliography[title={References}]

%-------------------------------------------------------------------------------
\end{document}   
%-------------------------------------------------------------------------------